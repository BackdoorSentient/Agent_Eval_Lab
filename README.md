Agent Eval Lab

A project for building and evaluating a multi-step reasoning agent with tools, using:
Langfuse â†’ Agent tracing & observability
DeepEval â†’ Behavioral & regression evaluation

------------------------------------------------------------------------------------------------------------------------------------------------
ğŸ¯ Purpose

This project demonstrates how to:
Build a multi-step tool-using agent
Trace full agent trajectories
Evaluate agent behavior (not just final answers)
Detect tool misuse
Score reasoning quality
Prevent regressions using CI

-------------------------------------------------------------------------------------------------------------------------------------------------
ğŸ§  Agent Architecture

User Input
   â†“
Planner (LLM)
   â†“
Tool Call(s)
   â†“
Observation
   â†“
Planner
   â†“
Final Answer

-------------------------------------------------------------------------------------------------------------------------------------------------
Each step is logged using Langfuse.
Evaluation is performed offline using DeepEval.

-------------------------------------------------------------------------------------------------------------------------------------------------
ğŸ“‚ Project Structure

agent-eval-lab/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ planner.py              # LLM planning logic
â”‚   â”œâ”€â”€ tools.py                # Tool implementations
â”‚   â”œâ”€â”€ executor.py             # Tool execution handler
â”‚   â””â”€â”€ agent.py                # Main agent loop
â”‚
â”œâ”€â”€ tracing/
â”‚   â””â”€â”€ langfuse_config.py      # Langfuse setup
â”‚
â”œâ”€â”€ evals/
â”‚   â”œâ”€â”€ dataset.py              # Evaluation task dataset
â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”œâ”€â”€ tool_usage.py
â”‚   â”‚   â”œâ”€â”€ tool_argument_accuracy.py
â”‚   â”‚   â”œâ”€â”€ reasoning_quality.py
â”‚   â”‚   â””â”€â”€ task_success.py
â”‚   â””â”€â”€ run_evals.py            # Batch evaluation runner
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_regression.py      # CI regression test
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

-------------------------------------------------------------------------------------------------------------------------------------------------
ğŸ— Evaluation Design

We evaluate:

1ï¸âƒ£ Task Success
Final answer correctness

2ï¸âƒ£ Tool Usage Accuracy
Did the agent choose the correct tools?

3ï¸âƒ£ Tool Argument Accuracy
Were correct arguments passed?

4ï¸âƒ£ Over-Tooling
Did the agent use unnecessary tools?

5ï¸âƒ£ Reasoning Quality
Evaluated using LLM-as-judge (GEval)

-------------------------------------------------------------------------------------------------------------------------------------------------
ğŸ“Š Dataset Categories

The evaluation dataset includes:
Single-step tool tasks
Multi-step reasoning tasks
No-tool tasks
Ambiguous tasks
Edge cases

This ensures meaningful evaluation coverage.

-------------------------------------------------------------------------------------------------------------------------------------------------
ğŸ” Observability

Langfuse logs:

Planning steps
Tool calls
Tool arguments
Tool outputs
Final answers
Latency & token usage

-------------------------------------------------------------------------------------------------------------------------------------------------
ğŸ“ˆ Evaluation Workflow

