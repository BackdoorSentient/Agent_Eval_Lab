# ğŸ§  Agent Eval Lab
Author-Aniket Waichal

<p align="center">
  <b>A Production-Style Multi-Step Agent Evaluation Lab</b><br>
  Built with Ollama, Langfuse, and DeepEval â€” Fully Local & Free
</p>

---

<p align="center">
  <img src="https://img.shields.io/badge/LLM-Ollama%20%7C%20Llama3-blue" />
  <img src="https://img.shields.io/badge/Tracing-Langfuse-orange" />
  <img src="https://img.shields.io/badge/Evaluation-DeepEval-green" />
  <img src="https://img.shields.io/badge/Testing-Pytest-yellow" />
  <img src="https://img.shields.io/badge/License-MIT-lightgrey" />
</p>

---

## ğŸš€ Overview

**Agent Eval Lab** Agent Eval Lab is a fully local framework for evaluating multi-step, tool-using LLM agents. It tracks execution traces, audits tool behavior, scores reasoning quality, and prevents regressions using automated tests.

It demonstrates how to:

- Build a tool-using reasoning agent
- Trace full execution trajectories
- Evaluate agent behavior (not just final answers)
- Detect tool misuse and over-tooling
- Score reasoning quality using LLM-as-judge
- Prevent regressions via automated testing

This project mirrors production-style agent evaluation workflows used in modern AI systems.

---
ğŸ§  Why This Project Is Valuable

This lab demonstrates understanding of:

- Agent orchestration
- Tool-augmented reasoning
- LLM-as-judge evaluation
- Observability-driven debugging
- Regression-safe AI systems

It reflects production-level agent evaluation design.

---
## ğŸ‘¥ Who Is This For?
- ML engineers building tool-using agents
- Researchers studying LLM reasoning
- Developers learning evaluation workflows

---

# ğŸ§  System Architecture

```text
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        User Input       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚      Planner (LLM)     â”‚
                â”‚   Ollama - llama3      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       Tool Calls        â”‚
                â”‚  calculator, converter  â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚       Observations      â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚     Final Answer        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
---
ğŸ” Observability Layer (Langfuse)

Each agent run is traced using self-hosted Langfuse.

We capture:

- Planning steps
- Tool selection
- Tool arguments
- Tool outputs
- Execution spans
- Final outputs

---
Langfuse runs locally via Docker:
```bash
http://localhost:3000
```
---

ğŸ“Š Evaluation Framework (DeepEval + Ollama)

All evaluation is performed locally using:
OllamaModel(model="llama3")

No OpenAI. No paid APIs. Fully offline.

---
ğŸ— Implemented Metrics
1ï¸âƒ£ Task Success

Final answer correctness.

2ï¸âƒ£ Tool Usage Accuracy

Did the agent choose the correct tool?

3ï¸âƒ£ Tool Argument Accuracy

Were correct arguments passed?

4ï¸âƒ£ Over-Tooling Detection

Did the agent use unnecessary tools?

5ï¸âƒ£ Reasoning Quality (GEval)

LLM-as-judge evaluation of reasoning coherence and logical planning.

---
ğŸ“‚ Project Structure
```text
agent-eval-lab/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ planner.py
â”‚   â”œâ”€â”€ tools.py
â”‚   â”œâ”€â”€ executor.py
â”‚   â””â”€â”€ agent.py
â”‚
â”œâ”€â”€ tracing/
â”‚   â””â”€â”€ langfuse_config.py
â”‚
â”œâ”€â”€ evals/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ metrics/
â”‚       â”œâ”€â”€ tool_usage.py
â”‚       â”œâ”€â”€ tool_argument_accuracy.py
â”‚       â”œâ”€â”€ reasoning_quality.py
â”‚       â””â”€â”€ task_success.py
â”‚   
â”‚
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_regression.py
â”‚
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
â””â”€â”€ run_evals.py
```
---
ğŸ¯ Evaluation Philosophy

Traditional evaluation checks only final outputs.

This lab evaluates:

- Behavioral correctness
- Tool decision quality
- Argument precision
- Multi-step reasoning integrity
- Regression stability

This shifts evaluation from output-only validation to trajectory-aware validation.

---
ğŸš€ Setup Guide
1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

2ï¸âƒ£ Start Ollama
```bash
ollama serve
ollama pull llama3
```

3ï¸âƒ£ Start Langfuse (Free & Local)
```bash
docker compose up
```

Open:
```bash
http://localhost:3000
```
Create a project and copy the generated keys.

4ï¸âƒ£ Configure Environment

Create .env:
```bash
LANGFUSE_PUBLIC_KEY=your_local_public_key
LANGFUSE_SECRET_KEY=your_local_secret_key
LANGFUSE_HOST=http://localhost:3000
```

---
ğŸ“ˆ Run Evaluations
python run_evals.py

---
ğŸ§ª Run Regression Tests
pytest

---

Tests fail if:
- Task success decreases
- Tool behavior changes
- Reasoning quality degrades

---
ğŸ”® Future Improvements

- Latency & performance metrics
- Hallucination detection
- Trajectory scoring benchmarks
- Multi-agent evaluation
- CI/CD integration (GitHub Actions)
- Experiment tracking layer

---